{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from math import log, sqrt, exp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import sign\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ftfy\n",
    "\n",
    "\n",
    "from sklearn.metrics import  classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Input, LSTM, Embedding, Dropout, Activation, MaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import gensim\n",
    "# import word2vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_tweets = pd.read_csv('datasets/depressed_tweets.csv', nrows = 40000)\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"tweet\"]\n",
    "random_tweets = pd.read_csv('datasets/nondepressive/training.1600000.processed.noemoticon.csv', encoding = \"ISO-8859-1\", names = DATASET_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wow, my dad yday: “you don’t take those stupid...</td>\n",
       "      <td>-0.2699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what part of this was really harmfult of a lot...</td>\n",
       "      <td>-0.5995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>see i wanna do one of them but they all say th...</td>\n",
       "      <td>-0.8643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IS IT clinical depression or is it the palpabl...</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Hope it's working for you.  I was on sertralin...</td>\n",
       "      <td>-0.3506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  vader_score  \\\n",
       "0           0  Wow, my dad yday: “you don’t take those stupid...      -0.2699   \n",
       "1           1  what part of this was really harmfult of a lot...      -0.5995   \n",
       "2           3  see i wanna do one of them but they all say th...      -0.8643   \n",
       "3           4  IS IT clinical depression or is it the palpabl...      -0.8316   \n",
       "4           7  Hope it's working for you.  I was on sertralin...      -0.3506   \n",
       "\n",
       "   vader_sentiment_label  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                              tweet  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = random_tweets.groupby(random_tweets.target)\n",
    "random_tweets = group.get_group(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweets = random_tweets[:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822272</td>\n",
       "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ersle</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822273</td>\n",
       "      <td>Mon Apr 06 22:22:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>becca210</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822283</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Wingman29</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822287</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>katarinka</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>4</td>\n",
       "      <td>1467822293</td>\n",
       "      <td>Mon Apr 06 22:22:46 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_EmilyYoung</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839995</th>\n",
       "      <td>4</td>\n",
       "      <td>1559610988</td>\n",
       "      <td>Sun Apr 19 11:35:35 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>alex_hernandez</td>\n",
       "      <td>Oficially done with drivers ed stuff. Now I ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839996</th>\n",
       "      <td>4</td>\n",
       "      <td>1559611013</td>\n",
       "      <td>Sun Apr 19 11:35:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>SusanCosmos</td>\n",
       "      <td>@jimhunt Thank YOU for sharing and caring!  Gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839997</th>\n",
       "      <td>4</td>\n",
       "      <td>1559611045</td>\n",
       "      <td>Sun Apr 19 11:35:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>hilzfuld</td>\n",
       "      <td>@buberzionist ok objection sustained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839998</th>\n",
       "      <td>4</td>\n",
       "      <td>1559611064</td>\n",
       "      <td>Sun Apr 19 11:35:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>missa_09</td>\n",
       "      <td>is twittin' while my baby girl sleeps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839999</th>\n",
       "      <td>4</td>\n",
       "      <td>1559611069</td>\n",
       "      <td>Sun Apr 19 11:35:36 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Gemmapple</td>\n",
       "      <td>but with facebook u can share loads more too.....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target         ids                          date      flag  \\\n",
       "800000       4  1467822272  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n",
       "800001       4  1467822273  Mon Apr 06 22:22:45 PDT 2009  NO_QUERY   \n",
       "800002       4  1467822283  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "800003       4  1467822287  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "800004       4  1467822293  Mon Apr 06 22:22:46 PDT 2009  NO_QUERY   \n",
       "...        ...         ...                           ...       ...   \n",
       "839995       4  1559610988  Sun Apr 19 11:35:35 PDT 2009  NO_QUERY   \n",
       "839996       4  1559611013  Sun Apr 19 11:35:36 PDT 2009  NO_QUERY   \n",
       "839997       4  1559611045  Sun Apr 19 11:35:36 PDT 2009  NO_QUERY   \n",
       "839998       4  1559611064  Sun Apr 19 11:35:36 PDT 2009  NO_QUERY   \n",
       "839999       4  1559611069  Sun Apr 19 11:35:36 PDT 2009  NO_QUERY   \n",
       "\n",
       "                  user                                              tweet  \n",
       "800000           ersle       I LOVE @Health4UandPets u guys r the best!!   \n",
       "800001        becca210  im meeting up with one of my besties tonight! ...  \n",
       "800002       Wingman29  @DaRealSunisaKim Thanks for the Twitter add, S...  \n",
       "800003       katarinka  Being sick can be really cheap when it hurts t...  \n",
       "800004     _EmilyYoung    @LovesBrooklyn2 he has that effect on everyone   \n",
       "...                ...                                                ...  \n",
       "839995  alex_hernandez  Oficially done with drivers ed stuff. Now I ju...  \n",
       "839996     SusanCosmos  @jimhunt Thank YOU for sharing and caring!  Gl...  \n",
       "839997        hilzfuld              @buberzionist ok objection sustained   \n",
       "839998        missa_09             is twittin' while my baby girl sleeps   \n",
       "839999       Gemmapple  but with facebook u can share loads more too.....  \n",
       "\n",
       "[40000 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>vader_score</th>\n",
       "      <th>vader_sentiment_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Wow, my dad yday: “you don’t take those stupid...</td>\n",
       "      <td>-0.2699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>what part of this was really harmfult of a lot...</td>\n",
       "      <td>-0.5995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>see i wanna do one of them but they all say th...</td>\n",
       "      <td>-0.8643</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IS IT clinical depression or is it the palpabl...</td>\n",
       "      <td>-0.8316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>Hope it's working for you.  I was on sertralin...</td>\n",
       "      <td>-0.3506</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>51189</td>\n",
       "      <td>I feel hopeless ,truely</td>\n",
       "      <td>-0.4588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>51191</td>\n",
       "      <td>Hello @CosmicYTT    People of Assam are suffer...</td>\n",
       "      <td>-0.5908</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>51193</td>\n",
       "      <td>Except for all that gerrymandering, voter supp...</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>51195</td>\n",
       "      <td>pantalón corto y camiseta (y el bañador debajo...</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>51197</td>\n",
       "      <td>@maybeabadbitch</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              tweet  \\\n",
       "0               0  Wow, my dad yday: “you don’t take those stupid...   \n",
       "1               1  what part of this was really harmfult of a lot...   \n",
       "2               3  see i wanna do one of them but they all say th...   \n",
       "3               4  IS IT clinical depression or is it the palpabl...   \n",
       "4               7  Hope it's working for you.  I was on sertralin...   \n",
       "...           ...                                                ...   \n",
       "39995       51189                            I feel hopeless ,truely   \n",
       "39996       51191  Hello @CosmicYTT    People of Assam are suffer...   \n",
       "39997       51193  Except for all that gerrymandering, voter supp...   \n",
       "39998       51195  pantalón corto y camiseta (y el bañador debajo...   \n",
       "39999       51197                                    @maybeabadbitch   \n",
       "\n",
       "       vader_score  vader_sentiment_label  \n",
       "0          -0.2699                      0  \n",
       "1          -0.5995                      0  \n",
       "2          -0.8643                      0  \n",
       "3          -0.8316                      0  \n",
       "4          -0.3506                      0  \n",
       "...            ...                    ...  \n",
       "39995      -0.4588                      0  \n",
       "39996      -0.5908                      0  \n",
       "39997      -0.6597                      0  \n",
       "39998      -0.2960                      0  \n",
       "39999       0.0000                      0  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depressed_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweets['label'] = [1]*40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed_tweets['label'] = [0]*40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tweets.drop(['target','ids','date','flag','user'], axis = 1, inplace = True)\n",
    "depressed_tweets.drop(['Unnamed: 0','vader_score','vader_sentiment_label'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([random_tweets,depressed_tweets], ignore_index  =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>I feel hopeless ,truely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>Hello @CosmicYTT    People of Assam are suffer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>Except for all that gerrymandering, voter supp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>pantalón corto y camiseta (y el bañador debajo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>@maybeabadbitch</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  label\n",
       "0           I LOVE @Health4UandPets u guys r the best!!       1\n",
       "1      im meeting up with one of my besties tonight! ...      1\n",
       "2      @DaRealSunisaKim Thanks for the Twitter add, S...      1\n",
       "3      Being sick can be really cheap when it hurts t...      1\n",
       "4        @LovesBrooklyn2 he has that effect on everyone       1\n",
       "...                                                  ...    ...\n",
       "79995                            I feel hopeless ,truely      0\n",
       "79996  Hello @CosmicYTT    People of Assam are suffer...      0\n",
       "79997  Except for all that gerrymandering, voter supp...      0\n",
       "79998  pantalón corto y camiseta (y el bañador debajo...      0\n",
       "79999                                    @maybeabadbitch      0\n",
       "\n",
       "[80000 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['I', 'LOVE', '@Health4UandPets', 'u', 'guys', 'r', 'the', 'best', '!', '!'],\n",
       "  0),\n",
       " (['im',\n",
       "   'meeting',\n",
       "   'up',\n",
       "   'with',\n",
       "   'one',\n",
       "   'of',\n",
       "   'my',\n",
       "   'besties',\n",
       "   'tonight',\n",
       "   '!',\n",
       "   'Cant',\n",
       "   'wait',\n",
       "   '!',\n",
       "   '!',\n",
       "   '-',\n",
       "   'GIRL',\n",
       "   'TALK',\n",
       "   '!',\n",
       "   '!'],\n",
       "  0),\n",
       " (['@DaRealSunisaKim',\n",
       "   'Thanks',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Twitter',\n",
       "   'add',\n",
       "   ',',\n",
       "   'Sunisa',\n",
       "   '!',\n",
       "   'I',\n",
       "   'got',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'you',\n",
       "   'once',\n",
       "   'at',\n",
       "   'a',\n",
       "   'HIN',\n",
       "   'show',\n",
       "   'here',\n",
       "   'in',\n",
       "   'the',\n",
       "   'DC',\n",
       "   'area',\n",
       "   'and',\n",
       "   'you',\n",
       "   'were',\n",
       "   'a',\n",
       "   'sweetheart',\n",
       "   '.'],\n",
       "  0),\n",
       " (['Being',\n",
       "   'sick',\n",
       "   'can',\n",
       "   'be',\n",
       "   'really',\n",
       "   'cheap',\n",
       "   'when',\n",
       "   'it',\n",
       "   'hurts',\n",
       "   'too',\n",
       "   'much',\n",
       "   'to',\n",
       "   'eat',\n",
       "   'real',\n",
       "   'food',\n",
       "   'Plus',\n",
       "   ',',\n",
       "   'your',\n",
       "   'friends',\n",
       "   'make',\n",
       "   'you',\n",
       "   'soup'],\n",
       "  0),\n",
       " (['@LovesBrooklyn2', 'he', 'has', 'that', 'effect', 'on', 'everyone'], 0)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tk = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "data = []\n",
    "\n",
    "\n",
    "X = df['tweet'].tolist()\n",
    "Y = df['label'].tolist()\n",
    "\n",
    "\n",
    "for x, y in zip(X, Y):\n",
    "    if y == 4:\n",
    "        data.append((tk.tokenize(x), 1))\n",
    "    else:\n",
    "        data.append((tk.tokenize(x), 0))\n",
    "      \n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\91810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def cleaned(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow' or token == '2moro':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token in ['hahah', 'hahaha', 'hahahaha']:\n",
    "        return 'haha'\n",
    "    if token == \"mother's\":\n",
    "        return \"mother\"\n",
    "    if token == \"mom's\":\n",
    "        return \"mom\"\n",
    "    if token == \"dad's\":\n",
    "        return \"dad\"\n",
    "    if token == 'bday' or token == 'b-day':\n",
    "        return 'birthday'\n",
    "    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\", \"they've\"]:\n",
    "        return token.replace(\"'\", \"\")\n",
    "    if token in ['lmao', 'lolz', 'rofl']:\n",
    "        return 'lol'\n",
    "    if token == '<3':\n",
    "        return 'love'\n",
    "    if token == 'thanx' or token == 'thnx' or token == \"thnks\":\n",
    "        return 'thanks'\n",
    "    if token == 'goood':\n",
    "        return 'good'\n",
    "    if token in ['amp', 'quot', 'lt', 'gt', '½25', '..', '. .', '. . .']:\n",
    "        return ''\n",
    "    return token\n",
    "\n",
    "\n",
    "\n",
    "def remove_noise(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token in tweet_tokens:\n",
    "        \n",
    "        token = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", token)\n",
    "        \n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        cleaned_token = cleaned(token.lower())\n",
    "        \n",
    "        if cleaned_token == \"idk\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('dont')\n",
    "            cleaned_tokens.append('know')\n",
    "            continue\n",
    "        if cleaned_token == \"i'll\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"you'll\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"we'll\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"it'll\":\n",
    "            cleaned_tokens.append('it')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"it's\":\n",
    "            cleaned_tokens.append('it')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"i've\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"you've\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"we've\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"they've\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"you're\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"we're\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"they're\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"let's\":\n",
    "            cleaned_tokens.append('let')\n",
    "            cleaned_tokens.append('us')\n",
    "            continue\n",
    "        if cleaned_token == \"she's\":\n",
    "            cleaned_tokens.append('she')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"he's\":\n",
    "            cleaned_tokens.append('he')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"that's\":\n",
    "            cleaned_tokens.append('that')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"i'd\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('would')\n",
    "            continue\n",
    "        if cleaned_token == \"you'd\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('would')\n",
    "            continue\n",
    "        if cleaned_token == \"there's\":\n",
    "            cleaned_tokens.append('there')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"what's\":\n",
    "            cleaned_tokens.append('what')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"how's\":\n",
    "            cleaned_tokens.append('how')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"who's\":\n",
    "            cleaned_tokens.append('who')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"y'all\" or cleaned_token == \"ya'll\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('all')\n",
    "            continue\n",
    "\n",
    "        if cleaned_token.strip() and cleaned_token not in string.punctuation and len(cleaned_token)>2 and cleaned_token not in STOP_WORDS: \n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "            \n",
    "    return cleaned_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('embeddings/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.3712e-01, -2.1691e-01, -6.6365e-03, -4.1625e-01, -1.2555e+00,\n",
       "       -2.8466e-02, -7.2195e-01, -5.2887e-01,  7.2085e-03,  3.1997e-01,\n",
       "        2.9425e-02, -1.3236e-02,  4.3511e-01,  2.5716e-01,  3.8995e-01,\n",
       "       -1.1968e-01,  1.5035e-01,  4.4762e-01,  2.8407e-01,  4.9339e-01,\n",
       "        6.2826e-01,  2.2888e-01, -4.0385e-01,  2.7364e-02,  7.3679e-03,\n",
       "        1.3995e-01,  2.3346e-01,  6.8122e-02,  4.8422e-01, -1.9578e-02,\n",
       "       -5.4751e-01, -5.4983e-01, -3.4091e-02,  8.0017e-03, -4.3065e-01,\n",
       "       -1.8969e-02, -8.5670e-02, -8.1123e-01, -2.1080e-01,  3.7784e-01,\n",
       "       -3.5046e-01,  1.3684e-01, -5.5661e-01,  1.6835e-01, -2.2952e-01,\n",
       "       -1.6184e-01,  6.7345e-01, -4.6597e-01, -3.1834e-02, -2.6037e-01,\n",
       "       -1.7797e-01,  1.9436e-02,  1.0727e-01,  6.6534e-01, -3.4836e-01,\n",
       "        4.7833e-02,  1.6440e-01,  1.4088e-01,  1.9204e-01, -3.5009e-01,\n",
       "        2.6236e-01,  1.7626e-01, -3.1367e-01,  1.1709e-01,  2.0378e-01,\n",
       "        6.1775e-01,  4.9075e-01, -7.5210e-02, -1.1815e-01,  1.8685e-01,\n",
       "        4.0679e-01,  2.8319e-01, -1.6290e-01,  3.8388e-02,  4.3794e-01,\n",
       "        8.8224e-02,  5.9046e-01, -5.3515e-02,  3.8819e-02,  1.8202e-01,\n",
       "       -2.7599e-01,  3.9474e-01, -2.0499e-01,  1.7411e-01,  1.0315e-01,\n",
       "        2.5117e-01, -3.6542e-01,  3.6528e-01,  2.2448e-01, -9.7551e-01,\n",
       "        9.4505e-02, -1.7859e-01, -3.0688e-01, -5.8633e-01, -1.8526e-01,\n",
       "        3.9565e-02, -4.2309e-01, -1.5715e-01,  2.0401e-01,  1.6906e-01,\n",
       "        3.4465e-01, -4.2262e-01,  1.9553e-01,  5.9454e-01, -3.0531e-01,\n",
       "       -1.0633e-01, -1.9055e-01, -5.8544e-01,  2.1357e-01,  3.8414e-01,\n",
       "        9.1499e-02,  3.8353e-01,  2.9075e-01,  2.4519e-02,  2.8440e-01,\n",
       "        6.3715e-02, -1.5483e-01,  4.0031e-01,  3.1543e-01, -3.7128e-02,\n",
       "        6.3363e-02, -2.7090e-01,  2.5160e-01,  4.7105e-01,  4.9556e-01,\n",
       "       -3.6401e-01,  1.0370e-01,  4.6076e-02,  1.6565e-01, -2.9024e-01,\n",
       "       -6.6949e-02, -3.0881e-01,  4.8263e-01,  3.0972e-01, -1.1145e-01,\n",
       "       -1.0329e-01,  2.8585e-02, -1.3579e-01,  5.2924e-01, -1.4077e-01,\n",
       "        9.1763e-02,  1.3127e-01, -2.0944e-01,  2.2327e-02, -7.7692e-02,\n",
       "        7.7934e-02, -3.3067e-02,  1.1680e-01,  3.2029e-01,  3.7749e-01,\n",
       "       -7.5679e-01, -1.5944e-01,  1.4964e-01,  4.2253e-01,  2.8136e-03,\n",
       "        2.1328e-01,  8.6776e-02, -5.2704e-02, -4.0859e-01, -1.1774e-01,\n",
       "        9.0621e-02, -2.3794e-01, -1.8326e-01,  1.3115e-01, -5.5949e-01,\n",
       "        9.2071e-02, -3.9504e-02,  1.3334e-01,  4.9632e-01,  2.8733e-01,\n",
       "       -1.8544e-01,  2.4618e-02, -4.2826e-01,  7.4148e-02,  7.6584e-04,\n",
       "        2.3950e-01,  2.2615e-01,  5.5166e-02, -7.5096e-02, -2.2308e-01,\n",
       "        2.3775e-01, -4.5455e-01,  2.6564e-01, -1.5137e-01, -2.4146e-01,\n",
       "       -2.4736e-01,  5.5214e-01,  2.6819e-01,  4.8831e-01, -1.3423e-01,\n",
       "       -1.5918e-01,  3.7606e-01, -1.9834e-01,  1.6699e-01, -1.5368e-01,\n",
       "        2.4561e-01, -9.2506e-02, -3.0257e-01, -2.9493e-01, -7.4917e-01,\n",
       "        1.0567e+00,  3.7971e-01,  6.9314e-01, -3.1672e-02,  2.1588e-01,\n",
       "       -4.0739e-01, -1.5264e-01,  3.2296e-01, -1.2999e-01, -5.0129e-01,\n",
       "       -4.4231e-01,  1.6904e-02, -1.1459e-02,  7.2293e-03,  1.1026e-01,\n",
       "        2.1568e-01, -3.2373e-01, -3.7292e-01, -9.2456e-03, -2.6769e-01,\n",
       "        3.9066e-01,  3.5742e-01, -6.0632e-02,  6.7966e-02,  3.3830e-01,\n",
       "        6.5747e-02,  1.5794e-01,  4.7155e-02,  2.3682e-01, -9.1370e-02,\n",
       "        6.4649e-01, -2.5491e-01, -6.7940e-01, -6.9752e-01, -1.0145e-01,\n",
       "       -3.6255e-01,  3.6967e-01, -4.1295e-01,  8.2724e-02, -3.5053e-01,\n",
       "       -1.7564e-01,  8.5095e-02, -5.7724e-01,  5.0252e-01,  5.2180e-01,\n",
       "        5.7327e-02, -7.9754e-01, -3.7770e-01,  7.8149e-01,  2.4597e-01,\n",
       "        6.0672e-01, -2.0082e-01, -3.8792e-01,  4.1295e-01, -1.6143e-01,\n",
       "        1.0427e-02,  4.3197e-01,  4.6297e-03,  2.1185e-01, -2.6606e-01,\n",
       "       -5.8740e-02, -5.1003e-01,  2.8524e-01,  1.3627e-02, -2.7346e-01,\n",
       "        6.1848e-02, -5.7901e-01, -5.1136e-01,  3.6382e-01,  3.5144e-01,\n",
       "       -1.6501e-01, -4.6041e-01, -6.4742e-02, -6.8310e-01, -4.7427e-02,\n",
       "        1.5861e-01, -4.7288e-01,  3.3968e-01,  1.2092e-03,  1.6018e-01,\n",
       "       -5.8024e-01,  1.4556e-01, -9.1317e-01, -3.7592e-01, -3.2950e-01,\n",
       "        5.3465e-01,  1.8224e-01, -5.2265e-01, -2.6209e-01, -4.2458e-01,\n",
       "       -1.8034e-01,  9.9502e-02, -1.5114e-01, -6.6731e-01,  2.4483e-01,\n",
       "       -5.6630e-01,  3.3843e-01,  4.0558e-01,  1.8073e-01,  6.4250e-01])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec_map['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
